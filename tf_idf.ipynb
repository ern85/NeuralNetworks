{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM6iK1i/ns2IuCRH/AbJeMs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ern85/NeuralNetworks/blob/main/tf_idf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtkyXvlTRDLU"
      },
      "source": [
        "#**Importações**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eviEctynxoLM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7549ac7-31ae-4ef1-e9b5-7495f723efbc"
      },
      "source": [
        "#importacoes\n",
        "import nltk #Natural Language Tool Kit\n",
        "import re # Regular Expressions\n",
        "nltk.download('rslp') #Removedor de Sufixos da Lingua Portuguesa\n",
        "from nltk import word_tokenize # Pacote para tokenização\n",
        "from nltk.stem import RSLPStemmer #Removedor de Sufixos da Língua Portuguesa\n",
        "nltk.download('punkt') # Pacote para alguns comandos de tokenização\n",
        "nltk.download('stopwords') #Retirada de palavras de parada\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aQRyeBg3dLz"
      },
      "source": [
        "# Entrada do texto que depois será uma chamada de arquivo\n",
        "texto = \"\"\"Em relação a compra não achei muito satisfatória pois mesmo com os produtos disponíveis no site informando estar a pronta entrega eles demoraram muito a chegar.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hteXrPYDQKut"
      },
      "source": [
        "# **Inciando o pré-processamento**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tVvyuJoxxCp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae0e08c0-08bc-4a75-c501-9ed4aff1f52c"
      },
      "source": [
        "# Remove pontuação\n",
        "texto = re.sub(r'[^\\w\\s]','',texto)\n",
        "print(texto)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Em relação a compra não achei muito satisfatória pois mesmo com os produtos disponíveis no site informando estar a pronta entrega eles demoraram muito a chegar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp8jde3vEmFi",
        "outputId": "7441b7ce-68c0-4920-f36c-56a96e5826f9"
      },
      "source": [
        "#tokenização de palavras\n",
        "\n",
        "dataset = nltk.word_tokenize(texto, language='portuguese')\n",
        "print (dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Em', 'relação', 'a', 'compra', 'não', 'achei', 'muito', 'satisfatória', 'pois', 'mesmo', 'com', 'os', 'produtos', 'disponíveis', 'no', 'site', 'informando', 'estar', 'a', 'pronta', 'entrega', 'eles', 'demoraram', 'muito', 'a', 'chegar']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setSxUcUJtth"
      },
      "source": [
        "#função Stemming\n",
        "def Stemming(sentence):\n",
        "    stemmer = RSLPStemmer()\n",
        "    phrase = []\n",
        "    for word in sentence:\n",
        "        phrase.append(stemmer.stem(word.lower()))\n",
        "    return phrase\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulQDzpQFN348",
        "outputId": "589f4ad7-ed82-4ccd-c496-282f25bda0db"
      },
      "source": [
        "#Chamada Stemming (para não ficar lento, chama a função)\n",
        "dataset = Stemming(dataset)\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['em', 'relaç', 'a', 'compr', 'não', 'ach', 'muit', 'satisfatór', 'poi', 'mesm', 'com', 'os', 'produt', 'dispon', 'no', 'sit', 'inform', 'est', 'a', 'pront', 'entreg', 'ele', 'demor', 'muit', 'a', 'cheg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dg_whhm7PlvF"
      },
      "source": [
        "#função Retirar Stopwords\n",
        "def RemoveStopWords(sentence):\n",
        "    stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "    phrase = []\n",
        "    for word in sentence:\n",
        "        if word not in stopwords:\n",
        "            phrase.append(word)\n",
        "    return phrase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKN1969FPprz",
        "outputId": "935b8cb2-4d4c-41aa-f42f-62241bf49a0f"
      },
      "source": [
        "#Chamada para retirar as stopwords\n",
        "dataset = RemoveStopWords(dataset)\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['relaç', 'compr', 'ach', 'muit', 'satisfatór', 'poi', 'mesm', 'produt', 'dispon', 'sit', 'inform', 'est', 'pront', 'entreg', 'demor', 'muit', 'cheg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF-TdIjzQbCq"
      },
      "source": [
        "# **Fim do pré-processamento**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vXhkRJfQ1_U"
      },
      "source": [
        "-------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1RNbJbqQoeD"
      },
      "source": [
        "# **Iniciando a preparação para os cálculos TF-IDF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79dr2uUu1AV_",
        "outputId": "01e480ef-1584-451d-ba14-9df58ff35b33"
      },
      "source": [
        "# criando um histograma\n",
        "\n",
        "word2count = {}\n",
        "for data in dataset:\n",
        "    words = nltk.word_tokenize(data)\n",
        "    for word in words:\n",
        "        if word not in word2count.keys():\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "            \n",
        "print (word2count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'relaç': 1, 'compr': 1, 'ach': 1, 'muit': 2, 'satisfatór': 1, 'poi': 1, 'mesm': 1, 'produt': 1, 'dispon': 1, 'sit': 1, 'inform': 1, 'est': 1, 'pront': 1, 'entreg': 1, 'demor': 1, 'cheg': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l_ntXYm1EtT",
        "outputId": "5e41b6b1-c544-47f7-e9ed-fa3d88b6e75c"
      },
      "source": [
        "# biblioteca para ordenação\n",
        "import heapq\n",
        "\n",
        "# ordena a lista para saber qual é a palavra que mais se repete\n",
        "freq_words = heapq.nlargest(50,word2count, key=word2count.get)\n",
        "print (freq_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['muit', 'relaç', 'compr', 'ach', 'satisfatór', 'poi', 'mesm', 'produt', 'dispon', 'sit', 'inform', 'est', 'pront', 'entreg', 'demor', 'cheg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co5n2bWy1Klq",
        "outputId": "67ed33e1-30c7-4db6-9ab7-91b813c2d13f"
      },
      "source": [
        "# criando o IDF\n",
        "import numpy as np \n",
        "\n",
        "word_idfs = {}\n",
        "for word in freq_words:\n",
        "    doc_count = 0 \n",
        "    for data in dataset:\n",
        "        if word in nltk.word_tokenize(data):\n",
        "            doc_count += 1\n",
        "    word_idfs[word] = np.log((len(dataset)/doc_count)+1)\n",
        "    \n",
        "print (word_idfs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'muit': 2.2512917986064953, 'relaç': 2.8903717578961645, 'compr': 2.8903717578961645, 'ach': 2.8903717578961645, 'satisfatór': 2.8903717578961645, 'poi': 2.8903717578961645, 'mesm': 2.8903717578961645, 'produt': 2.8903717578961645, 'dispon': 2.8903717578961645, 'sit': 2.8903717578961645, 'inform': 2.8903717578961645, 'est': 2.8903717578961645, 'pront': 2.8903717578961645, 'entreg': 2.8903717578961645, 'demor': 2.8903717578961645, 'cheg': 2.8903717578961645}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqMLjdAS1OUK",
        "outputId": "c608e1ff-0b4d-4945-caf5-bea25ac5123b"
      },
      "source": [
        "# calculando a frequencia de cada palavra no documento\n",
        "\n",
        "tf_matrix = {}\n",
        "print(dataset)\n",
        "for word in freq_words:\n",
        "    doc_tf = []\n",
        "    for data in dataset:\n",
        "        frequency = 0\n",
        "        for w in nltk.word_tokenize(data):\n",
        "            if w == word:\n",
        "                frequency += 1\n",
        "        tf_word = frequency/len(nltk.word_tokenize(data))\n",
        "        doc_tf.append(tf_word)\n",
        "    tf_matrix[word] = doc_tf\n",
        "\n",
        "print (tf_matrix)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['relaç', 'compr', 'ach', 'muit', 'satisfatór', 'poi', 'mesm', 'produt', 'dispon', 'sit', 'inform', 'est', 'pront', 'entreg', 'demor', 'muit', 'cheg']\n",
            "{'muit': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], 'relaç': [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'compr': [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'ach': [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'satisfatór': [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'poi': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'mesm': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'produt': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'dispon': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'sit': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'inform': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'est': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'pront': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0], 'entreg': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], 'demor': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], 'cheg': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZldjFR251SQF",
        "outputId": "470b2309-b714-4c82-8d32-6d3c67996dbc"
      },
      "source": [
        "# TF-IDF cálculo\n",
        "\n",
        "tfidf_matrix = []\n",
        "print(freq_words)\n",
        "for word in tf_matrix.keys():\n",
        "    tfidf = []\n",
        "    for value in tf_matrix[word]:\n",
        "        score = value * word_idfs[word]\n",
        "        tfidf.append(score)\n",
        "    tfidf_matrix.append(word)    \n",
        "    tfidf_matrix.append(tfidf)\n",
        "    \n",
        "#print (tfidf_matrix)\n",
        "\n",
        "for i in range(0, len(tfidf_matrix), 2):\n",
        "    print(tfidf_matrix[i:i+2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['muit', 'relaç', 'compr', 'ach', 'satisfatór', 'poi', 'mesm', 'produt', 'dispon', 'sit', 'inform', 'est', 'pront', 'entreg', 'demor', 'cheg']\n",
            "['muit', [0.0, 0.0, 0.0, 2.2512917986064953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.2512917986064953, 0.0]]\n",
            "['relaç', [2.8903717578961645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "['compr', [0.0, 2.8903717578961645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "['ach', [0.0, 0.0, 2.8903717578961645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "['satisfatór', [0.0, 0.0, 0.0, 0.0, 2.8903717578961645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "['poi', [0.0, 0.0, 0.0, 0.0, 0.0, 2.8903717578961645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "['mesm', [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8903717578961645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "['produt', [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8903717578961645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "['dispon', [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8903717578961645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "['sit', [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8903717578961645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "['inform', [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8903717578961645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "['est', [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8903717578961645, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "['pront', [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8903717578961645, 0.0, 0.0, 0.0, 0.0]]\n",
            "['entreg', [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8903717578961645, 0.0, 0.0, 0.0]]\n",
            "['demor', [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8903717578961645, 0.0, 0.0]]\n",
            "['cheg', [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.8903717578961645]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}